{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee216ac-cbc1-4576-a6c2-74e6ae2ff2c9",
   "metadata": {},
   "source": [
    "# 6.4: Resampling Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451fa372-7eb2-43e8-bbf5-429bc446f0a0",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "### Import Libraries \n",
    "\n",
    "We import our standard libraries and specific objects/libraries at the top level of our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edbc31d-be7c-4dd3-bb07-a9f681754433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our previous libraries and objects\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import ModelSpec as MS\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import new libraries and objects\n",
    "from sklearn.model_selection import cross_validate\n",
    "from ISLP.models import sklearn_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f01389-fd9e-4824-8601-44ce42f58f98",
   "metadata": {},
   "source": [
    "## The Validation Set Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e417696-f8c8-4ef5-8404-e6838a907d47",
   "metadata": {},
   "source": [
    "We explore the use of the validation set approach in order to estimate the test error rates that result from fitting various linear models on the `Auto` data set.\n",
    "\n",
    "We use the function `train_test_split()` to split the data into training and validation sets. As there are 392 observations, we split into two equal sets of size 196 using the argument `test_size=196`. We want to set a random seed (`random_state=0`) so that the random results can be reproduced again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0419abe2-c733-4124-b202-45f6a8bf953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto = load_data('Auto')\n",
    "Auto_train, Auto_valid = train_test_split(Auto,\n",
    "                                         test_size=196, # split in two\n",
    "                                         random_state=0) # random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6667129-56af-4d42-9416-2b51e60e3f0c",
   "metadata": {},
   "source": [
    "Now we can fit a linear regression using only the observations corresponding to the training set `Auto_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c531b5b-803b-42ed-88f0-e20242e0ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_mm = MS(['horsepower'])\n",
    "X_train = hp_mm.fit_transform(Auto_train)\n",
    "y_train = Auto_train['mpg']\n",
    "model = sm.OLS(y_train, X_train)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af93fbe6-df41-4f9f-878a-8b58b38fd0a0",
   "metadata": {},
   "source": [
    "We now use the `predict()` method of results evaluated on the model matrix for this model created using the validation data set. We also calculate the validation MSE of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda5e219-dcd0-4569-a6b3-02954d68d2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = hp_mm.transform(Auto_valid)\n",
    "y_valid = Auto_valid['mpg']\n",
    "valid_pred = results.predict(X_valid)\n",
    "np.mean((y_valid - valid_pred)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252d8d71-405e-4b5f-92ea-365e44b2a8c1",
   "metadata": {},
   "source": [
    "So our estimated test MSE for the linear regression is 23.62.\n",
    "\n",
    "**What would happen if we used a different set of observations for our training set? Repeat the above\n",
    "process using a different seed (i.e. set.seed(2)) and report the differences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c467449e-0c6d-413d-8f2c-97dccfbc5e5d",
   "metadata": {},
   "source": [
    "## Leave-One-Out Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa9709e-556e-4c23-81c8-3b75dd28c9a7",
   "metadata": {},
   "source": [
    "The simplest way to cross-validate in Python is to use `sklearn`. The `ISLP` package provides a wrapper (`sklearn_sm()`), that allows us to easily use the cross-validation tools of `sklearn` with models fit by `statsmodels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22117be0-29ae-4ccb-9b93-c5c538e90528",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_model = sklearn_sm(sm.OLS, MS(['horsepower']))\n",
    "X, Y = Auto.drop(columns=['mpg']), Auto['mpg']\n",
    "cv_results = cross_validate(hp_model,\n",
    "                            X,\n",
    "                            Y,\n",
    "                            cv=Auto.shape[0])\n",
    "cv_err = np.mean(cv_results['test_score'])\n",
    "cv_err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45839a95-4991-43d4-bc15-a529d0121252",
   "metadata": {},
   "source": [
    "The arguments to `cross_validate()` are: an object with the appropriate `fit()`, `predict()`, and `score()` methods, an array of features `X` and a response `Y`. We also included an additional argument `cv` to `cross_validate()`; specifying an integer \n",
    "$K$ results in $K$-fold cross-validation.\n",
    "\n",
    "We can repeat this procedure for increasingly complex polynomial fits. To automate the process, we again use a for loop which iteratively fits polynomial regressions of degree 1 to 5, computes the associated cross-validation error, and stores it in the $i$th element of the vector `cv_error`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ef2032-b30d-40e7-8e43-ede2f12c5fbd",
   "metadata": {},
   "source": [
    "**Write a for loop that fits the Auto data using polynomials up to degree 5. Compute the LOOCV\n",
    "test error rate for each of them. Which degree polynomial fits the data the best according to\n",
    "the test error rate and the bias-variance trade-off?**\n",
    "\n",
    "Tip: use `np.power.outer` function to construct `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752e078f-f2a6-40c9-9597-e45a0698b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278c6959-9713-480a-bbe7-b58502a9d49f",
   "metadata": {},
   "source": [
    "*These exercises were adapted from :* James, Gareth, et al. An Introduction to Statistical Learning: with Applications in Python, Springer, 2023."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi",
   "language": "python",
   "name": "dsi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
